# -*- coding: utf-8 -*-
"""
Created on Thu Oct  8 15:17:42 2020

@author: daxiguammm
"""

#多层感知器
'''
输入层-隐含层-输出层-输出
'''

#梯度下降算法

#学习速率
'''
学习速率是一种超参数或对模型的一种手工可配置的设置
需要为它指定正确的值。太小则需要许多轮迭代；太大则可能”“跳过极小值点并且因周期性”
跳过“而永远无法找到极小值点
'''

#反向传播算法
'''
是一种高效计算数据流图中梯度的技术
每一层的导数都是后一层的导数与前一层输出之积，
这正是链式法则的奇妙之处，误差反向传播算法利用的正是这一特点

前馈时（向前传播),从输入开始，逐一计算每个隐含层的输出，直到输出层。

然后开始计算导数，并从输出层经各隐含层逐一反向传播。为了减少计算量，还需对所有已完成计算的元素进行复用。
这便是反向传播算法的名称的由来。
'''

#常见的优化函数
'''
优化器时编译模型的所需的两个参数之一。
你可以先实例化一个优化器对象，然后将它传入model.compile(),
或者你可以通过名称来调用优化器。在后一种情况下，将使用优化器的默认参数。

SGD:随机梯度下降优化器
随机梯度下降优化器SGD和min-batch时同一个意思，抽取
m个小批量（独立同分布）样本，通过计算他们的平梯度均值。

RMSprop：经验上，RMSProp被证明有效且实用的深度学习网络优化算法
RMSprop增加了一个衰减系数来控制历史信息的获取多少
RMSprop会对学习率进行衰减

Adam优化器：
1、可以看作时修改后Momentum+RMSProp算法
2、Adam通常被认为对超参数的选择相当鲁棒
3、学习率建议为0.001
'''
#训练
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
              loss='sparse_categorical_crossentropy',#顺序标签使用这个
              metrics=['acc']
)










